#!/usr/bin/env python3
"""
LinkedIn Content Automation Script

This script replicates the functionality of the n8n workflow for generating
LinkedIn posts based on research from YouTube and Twitter/X.
"""

import os
import json
import requests
from typing import List, Dict, Any
from dataclasses import dataclass
from datetime import datetime
import argparse
from googleapiclient.discovery import build
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.http import MediaIoBaseUpload
import io

# Parse command line arguments at the global scope
parser = argparse.ArgumentParser(description='LinkedIn Content Automation')
parser.add_argument('--skip-review', action='store_true', help='Skip human review step')

# Parse arguments and make them available as global variables
args = parser.parse_args()
SKIP_REVIEW = args.skip_review

# Configuration - Replace with your actual API keys and credentials
CONFIG = {
    "apify_api_key": os.environ.get("APIFY_API_KEY"),
    "openrouter_api_key": os.environ.get("OPENROUTER_API_KEY"),
    "google_drive_folder_id": os.environ.get("GOOGLE_DRIVE_FOLDER_ID"),
    "youtube_search_query": "your_search_query_here",
    "twitter_username": os.environ.get("TWITTER_USERNAME"),  # Replace with actual username
    "max_results": 10
}

# Google Drive API scopes
SCOPES = ['https://www.googleapis.com/auth/drive.file']


@dataclass
class ResearchData:
    """Class for storing research data from various sources"""
    youtube_videos: List[Dict[str, Any]]
    youtube_transcripts: List[Dict[str, Any]]
    twitter_posts: List[Dict[str, Any]]
    combined_research: Dict[str, Any] = None


class APIClient:
    """Handles API requests to external services"""
    
    @staticmethod
    def fetch_youtube_videos(query: str, max_results: int) -> List[Dict[str, Any]]:
        """Fetch YouTube videos using Apify API"""
        url = "https://api.apify.com/v2/acts/streamers~youtube-scraper/run-sync-get-dataset-items"
        
        payload = {
            "dateFilter": "month",
            "downloadSubtitles": False,
            "hasCC": False,
            "hasLocation": False,
            "hasSubtitles": False,
            "is360": False,
            "is3D": False,
            "is4K": False,
            "isBought": False,
            "isHD": False,
            "isHDR": False,
            "isLive": False,
            "isVR180": False,
            "lengthFilter": "between420",
            "maxResultStreams": 0,
            "maxResults": max_results,
            "maxResultsShorts": 10,
            "preferAutoGeneratedSubtitles": True,
            "saveSubsToKVS": True,
            "searchQueries": [query],
            "sortingOrder": "views",
            "subtitlesLanguage": "en",
            "videoType": "video"
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {CONFIG['apify_api_key']}"
        }
        
        response = requests.post(url, json=payload, headers=headers)
        
        if response.status_code == 200:
            return response.json()
        else:
            print(f"Error fetching YouTube videos: {response.status_code}")
            print(response.text)
            return []

    @staticmethod
    def fetch_youtube_transcript(video_url: str) -> Dict[str, Any]:
        """Fetch transcript for a YouTube video"""
        url = "https://api.apify.com/v2/acts/pintostudio~youtube-transcript-scraper/run-sync-get-dataset-items"
        
        payload = {
            "videoUrl": video_url
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {CONFIG['apify_api_key']}"
        }
        
        response = requests.post(url, json=payload, headers=headers)
        
        if response.status_code == 200:
            return response.json()
        else:
            print(f"Error fetching YouTube transcript: {response.status_code}")
            print(response.text)
            return {}

    @staticmethod
    def fetch_twitter_posts(username: str, max_posts: int) -> List[Dict[str, Any]]:
        """Fetch Twitter/X posts using Apify API"""
        url = "https://api.apify.com/v2/acts/danek~twitter-scraper-ppr/run-sync-get-dataset-items"
        
        payload = {
            "includeReplies": False,
            "includeRetweets": False,
            "max_posts": max_posts,
            "username": username
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {CONFIG['apify_api_key']}"
        }
        
        response = requests.post(url, json=payload, headers=headers)
        
        if response.status_code == 200:
            return response.json()
        else:
            print(f"Error fetching Twitter posts: {response.status_code}")
            print(response.text)
            return []


class ContentProcessor:
    """Processes and transforms content from various sources"""
    
    @staticmethod
    def extract_transcript_text(transcript_data: List[Dict[str, Any]]) -> str:
        """Extract and join text from transcript data"""
        if not transcript_data or not isinstance(transcript_data, list):
            return ""
        
        try:
            full_caption = " ".join([item.get("text", "") for item in transcript_data])
            return full_caption
        except Exception as e:
            print(f"Error extracting transcript text: {e}")
            return ""

    @staticmethod
    def combine_research_data(videos: List[Dict[str, Any]], 
                             transcripts: List[Dict[str, Any]], 
                             tweets: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Combine research data from different sources"""
        combined_data = {
            "youtube_data": videos,
            "transcript_data": transcripts,
            "twitter_data": tweets,
            "timestamp": datetime.now().isoformat()
        }
        
        return combined_data


class AIProcessor:
    """Handles AI processing using OpenRouter API"""
    
    @staticmethod
    def generate_research(research_data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate research using Claude AI"""
        url = "https://openrouter.ai/api/v1/chat/completions"
        
        prompt = f"""
        Using the following data as a starting point:
        
        {json.dumps(research_data, indent=2)}
        
        Research and find marketing use cases - around how marketers can scale marketing efforts with AI using n8n for specific marketing use case like ad campaign, scaling outbound, scaling content creation.
        
        Please conduct deep research on the topic above. Specifically:
        
        - Key Trends and Insights
        - Marketing use cases
        - What's currently happening in the industry or niche?
        - Are there stats, frameworks, or case studies worth referencing?
        - Popular Opinions vs. Expert Takes
        - What are people saying on social platforms or forums?
        - Are there any contrarian, expert-backed, or field-tested perspectives?
        - Data, Stats, or Real Examples
        - Include any performance benchmarks, studies, or business use cases
        - Source Links or Summarized Citations
        - If quoting or citing, include the origin (author, source, link)
        
        Output format:
        
        a. topic
        b. key insights
        c. expert takes
        d. Supporting data with source
        e. citations
        f. use cases
        """
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {CONFIG['openrouter_api_key']}"
        }
        
        payload = {
            "model": "anthropic/claude-3.7-sonnet",
            "messages": [
                {"role": "system", "content": "You are a marketing research assistant."},
                {"role": "user", "content": prompt}
            ]
        }
        
        response = requests.post(url, json=payload, headers=headers)
        
        if response.status_code == 200:
            result = response.json()
            content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            return {"research_output": content}
        else:
            print(f"Error generating research: {response.status_code}")
            print(response.text)
            return {"research_output": "Error generating research"}

    @staticmethod
    def generate_linkedin_post(research_output: Dict[str, Any]) -> Dict[str, Any]:
        """Generate LinkedIn post using Perplexity AI"""
        url = "https://openrouter.ai/api/v1/chat/completions"
        
        brand_voice = """
        Brand voice to follow:
        
        - Write with authentic expertise and direct communication
        - Use confident, straightforward language demonstrating real experience
        - Be intelligent without academic or overly formal phrasing
        - Speak directly to the reader as if sharing insider knowledge
        - Avoid corporate jargon and marketing-speak
        - Use contractions and occasional casual phrases to maintain authenticity
        - Make definitive statements rather than hedging
        - Balance technical accuracy with accessibility
        - Structure content with clear, punchy headers and concise explanations
        - Prioritize actionable advice over theoretical concepts
        - Sound like a successful practitioner sharing hard-earned wisdom
        - Avoid both overly casual language ("stuff," "randos") and artificially elevated vocabulary ("magnetizes elite collaborators")
        - Use precise, impactful language that respects reader intelligence while remaining accessible
        """
        
        prompt = f"""
        {brand_voice}
        
        Instructions:
        
        Use research as context only:
        {json.dumps(research_output, indent=2)}
        
        Create a narrative driven post that gives readers actionable insights and takeaways and impacts business in positive ways (only use data if it's relevant, don't pick topics around AI marketing tools blindly).
        
        Write one detailed (400 words) LinkedIn post focused on how marketers can scale marketing efforts with AI using n8n for a specific marketing use case like ad campaign, scaling outbound, or scaling content creation.
        
        - Avoid sharing hashtags
        - Create 1 detailed post (400 words)
        - Topic - Pick topic around how marketers can scale marketing efforts with AI using n8n for specific marketing use case
        
        Output format:
        - Title
        - Content
        """
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {CONFIG['openrouter_api_key']}"
        }
        
        payload = {
            "model": "perplexity/sonar",
            "messages": [
                {"role": "system", "content": "You are a LinkedIn content strategist and conversion copywriter."},
                {"role": "user", "content": prompt}
            ]
        }
        
        response = requests.post(url, json=payload, headers=headers)
        
        if response.status_code == 200:
            result = response.json()
            content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            return {"linkedin_post": content}
        else:
            print(f"Error generating LinkedIn post: {response.status_code}")
            print(response.text)
            return {"linkedin_post": "Error generating LinkedIn post"}


class DocumentFormatter:
    """Formats content for Google Docs"""
    
    @staticmethod
    def format_content(data: List[Dict[str, Any]]) -> str:
        """Format content for Google Docs"""
        content = "# Research\n\n"
        
        for section in data:
            content += f"## {section.get('section', '')}\n"
            
            for key, value in section.items():
                if key != "section":
                    if isinstance(value, list):
                        content += f"- **{key.replace('_', ' ')}**:\n  - " + "\n  - ".join(value) + "\n\n"
                    else:
                        content += f"- **{key.replace('_', ' ')}**: {value}\n\n"
        
        return content.strip()


class GoogleDriveUploader:
    """Handles uploading content to Google Drive"""
    
    @staticmethod
    def get_credentials():
        """Get Google Drive credentials"""
        creds = None
        token_path = 'token.json'
        credentials_path = 'credentials.json'
        
        if os.path.exists(token_path):
            creds = Credentials.from_authorized_user_info(json.load(open(token_path)))
            
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                flow = InstalledAppFlow.from_client_secrets_file(credentials_path, SCOPES)
                creds = flow.run_local_server(port=0)
            
            with open(token_path, 'w') as token:
                token.write(creds.to_json())
                
        return creds

    @staticmethod
    def upload_to_drive(content: str, title: str, folder_id: str) -> str:
        """Upload content to Google Drive"""
        try:
            creds = GoogleDriveUploader.get_credentials()
            service = build('drive', 'v3', credentials=creds)
            
            file_metadata = {
                'name': title,
                'mimeType': 'application/vnd.google-apps.document',
                'parents': [folder_id]
            }
            
            media = MediaIoBaseUpload(
                io.BytesIO(content.encode('utf-8')),
                mimetype='text/plain',
                resumable=True
            )
            
            file = service.files().create(
                body=file_metadata,
                media_body=media,
                fields='id'
            ).execute()
            
            print(f"File ID: {file.get('id')}")
            return file.get('id')
            
        except Exception as e:
            print(f"Error uploading to Google Drive: {e}")
            return None


class HumanReview:
    """Handles human review of generated content"""
    
    @staticmethod
    def prompt_for_review(content: Dict[str, Any]) -> bool:
        """Prompt for human review and approval"""
        print("\n" + "="*50)
        print("GENERATED LINKEDIN POST FOR REVIEW")
        print("="*50 + "\n")
        
        print(json.dumps(content, indent=2))
        
        while True:
            response = input("\nApprove this content? (yes/no): ").lower().strip()
            if response in ['yes', 'y']:
                return True
            elif response in ['no', 'n']:
                return False
            else:
                print("Please enter 'yes' or 'no'")


class WorkflowOrchestrator:
    """Orchestrates the entire workflow"""
    
    def __init__(self):
        self.api_client = APIClient()
        self.content_processor = ContentProcessor()
        self.ai_processor = AIProcessor()
        self.document_formatter = DocumentFormatter()
        self.drive_uploader = GoogleDriveUploader()
        self.human_review = HumanReview()
    
    def run_workflow(self):
        """Run the complete workflow"""
        print("Starting LinkedIn content automation workflow...")
        
        # 1. Collect data from YouTube
        print("Collecting YouTube videos...")
        youtube_videos = self.api_client.fetch_youtube_videos(
            CONFIG['youtube_search_query'], 
            CONFIG['max_results']
        )
        
        # 2. Collect transcripts for each video
        print("Collecting YouTube transcripts...")
        youtube_transcripts = []
        for video in youtube_videos:
            video_url = video.get('url')
            if video_url:
                transcript = self.api_client.fetch_youtube_transcript(video_url)
                if transcript:
                    # Extract and join text
                    full_caption = self.content_processor.extract_transcript_text(transcript.get('data', []))
                    youtube_transcripts.append({"video_url": video_url, "full_caption": full_caption})
        
        # 3. Collect data from Twitter/X
        print("Collecting Twitter posts...")
        twitter_posts = self.api_client.fetch_twitter_posts(
            CONFIG['twitter_username'], 
            CONFIG['max_results']
        )
        
        # 4. Combine research data
        print("Combining research data...")
        combined_data = self.content_processor.combine_research_data(
            youtube_videos, 
            youtube_transcripts, 
            twitter_posts
        )
        
        # 5. Generate research using AI
        print("Generating research using AI...")
        research_output = self.ai_processor.generate_research(combined_data)
        
        # 6. Generate LinkedIn post
        print("Generating LinkedIn post...")
        linkedin_post = self.ai_processor.generate_linkedin_post(research_output)
        
        # 7. Human review
        print("Awaiting human review...")
        approved = self.human_review.prompt_for_review(linkedin_post)
        
        if not approved:
            print("Content not approved. Workflow terminated.")
            return
        
        # 8. Format content for Google Docs
        print("Formatting content for Google Docs...")
        formatted_content = linkedin_post.get('linkedin_post', '')
        
        # 9. Upload to Google Drive
        print("Uploading to Google Drive...")
        file_id = self.drive_uploader.upload_to_drive(
            formatted_content,
            f"LinkedIn Post - {datetime.now().strftime('%Y-%m-%d')}",
            CONFIG['google_drive_folder_id']
        )
        
        if file_id:
            print(f"Workflow completed successfully. File uploaded to Google Drive with ID: {file_id}")
        else:
            print("Workflow completed but file upload failed.")


def main():
    """Main function to run the workflow"""
    # Use the global SKIP_REVIEW variable instead of parsing arguments here
    
    # Override human review if specified
    if SKIP_REVIEW:
        HumanReview.prompt_for_review = lambda self, content: True
    
    # Run the workflow
    orchestrator = WorkflowOrchestrator()
    orchestrator.run_workflow()


if __name__ == "__main__":
    main()
